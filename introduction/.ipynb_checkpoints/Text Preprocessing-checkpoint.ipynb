{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "In this notebook we look into basic steps which are involved in any NLP task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"i love python and natural language processing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Tokenization is a process of converting text into a list of words or phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'love', 'python', 'and', 'natural', 'language', 'processing']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenization\n",
    "text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also use NLTK's tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'love', 'python', 'and', 'natural', 'language', 'processing']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "Lemmatization is basically to convert a word to it's root or base word format which is also known as lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpora  :  corpus\n",
      "books  :  book\n",
      "going  :  going\n",
      "better  :  better\n"
     ]
    }
   ],
   "source": [
    "# choose some words to be lemmatized \n",
    "words = [\"corpora\", \"books\", \"going\",\"better\"] \n",
    "  \n",
    "for w in words: \n",
    "    print(w, \" : \", lemmatizer.lemmatize(w)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better : good\n",
      "going : go\n"
     ]
    }
   ],
   "source": [
    "#Can we make better BETTER?\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\")) \n",
    "#pos \"a\" denotes adjective\n",
    "\n",
    "print(\"going :\", lemmatizer.lemmatize(\"going\", pos =\"v\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "Stemming is a process to convert words into it's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program  :  program\n",
      "prgramming  :  prgram\n",
      "programmer  :  programm\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer a wo be stemmed o\n",
    "for w in words: \n",
    "    print(w, \" : \", ps.stem(w)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But its not always perfect because it tries to cut down the suffixes and prefixes based on certain language rule. Let see one more example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "universe  :  univers\n",
      "university  :  univers\n"
     ]
    }
   ],
   "source": [
    "new_words = [\"universe\",\"university\"]\n",
    "for w in new_words: \n",
    "    print(w, \" : \", ps.stem(w)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words\n",
    "Stop words are generally the top frequency words which don't have any information associated with them e.g., The, Is, A, An etc \n",
    "The definition of stop word can be very specific to your application. e.g., if you are trying to predict whether a word is singular or plural - You might not want to remove tokens like 'IS', 'ARE' etc.\n",
    "\n",
    "Here we will use NLTK's stop words and see how does it work\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stp = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stp[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_split = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_split_without_sp = []\n",
    "for i in range(len(text_split)):\n",
    "    if(text_split[i] not in stp):\n",
    "        text_split_without_sp.append(text_split[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love', 'python', 'natural', 'language', 'processing']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_split_without_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'love', 'python', 'and', 'natural', 'language', 'processing']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using fast.ai\n",
    "Fast.ai contains very useful functions for NLP, we will explore their text.transform\n",
    "\n",
    "The rules are all listed below, here is the meaning of the special tokens:\n",
    "\n",
    "- UNK (xxunk) is for an unknown word (one that isn't present in the current vocabulary)\n",
    "- PAD (xxpad) is the token used for padding, if we need to regroup several texts of different lengths in a batch\n",
    "- BOS (xxbos) represents the beginning of a text in your dataset\n",
    "- FLD (xxfld) is used if you set mark_fields=True in your TokenizeProcessor to separate the different fields of texts (if your texts are loaded from several columns in a dataframe)\n",
    "- TK_MAJ (xxmaj) is used to indicate the next word begins with a capital in the original text\n",
    "- TK_UP (xxup) is used to indicate the next word is written in all caps in the original text\n",
    "- TK_REP (xxrep) is used to indicate the next character is repeated n times in the original text (usage xxrep n {char})\n",
    "- TK_WREP(xxwrep) is used to indicate the next word is repeated n times in the original text (usage xxwrep n {word})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import Tokenizer\n",
    "from fastai.text import SpacyTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i love python and natural language processing'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tok = SpacyTokenizer('en')\n",
    "' '.join(tokenizer.process_text(text, tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"I WANT it Today !!!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i xxup want it xxmaj today xxrep 4 !'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = SpacyTokenizer('en')\n",
    "' '.join(tokenizer.process_text(text2, tok))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gpu]",
   "language": "python",
   "name": "conda-env-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
